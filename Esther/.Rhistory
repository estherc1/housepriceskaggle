cormodel.fullhouse.m.or[1444,]
x[1444,]
x=model.matrix(log(SalePrice)~.-1,data = cormodel.fullhouse.m.or)
dim(x)
2172-1444
colnames(house.c.woSalePrice)[89]
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.house.c = tree(log(SalePrice) ~ ., stepmodel.fullhouse.m.or, subset = train)
summary(tree.house.c)
#Visually inspecting the regression tree.
plot(tree.house.c)
text(tree.house.c, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.house.c = cv.tree(tree.house.c)
par(mfrow = c(1, 2))
plot(cv.house.c$size, cv.house.c$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.house.c$k, cv.house.c$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.house.c = prune.tree(tree.house.c, best = 10)
par(mfrow = c(1, 1))
plot(prune.house.c)
text(prune.house.c, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat= predict(tree.house.c, newdata = house.c.modeldata[-train, ])
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
plot(yhat, house.c.test)
abline(0, 1)
mean((yhat - house.c.test)^2, na.rm = TRUE)
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
house.c.test
#Calculating and assessing the MSE of the test data on the overall tree.
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
house.c.test
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
prediction2 = yhat[439:length(house.c.test)]
prediction2 = yhat[435:length(house.c.test)]
submission2 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission2,"submit2.csv")
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
prediction3 = yhat[435:length(house.c.test)]
submission3 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission3,"submit3.csv",row.names = FALSE)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
prediction3 = yhat[435:length(house.c.test)]
submission3 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission3,"submit3.csv",row.names = FALSE)
tree.house.c = tree(log(SalePrice) ~ ., stepmodel.fullhouse.m.or, subset = train)
summary(tree.house.c)
#Visually inspecting the regression tree.
plot(tree.house.c)
text(tree.house.c, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.house.c = cv.tree(tree.house.c)
par(mfrow = c(1, 2))
plot(cv.house.c$size, cv.house.c$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.house.c$k, cv.house.c$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.house.c = prune.tree(tree.house.c, best = 10)
par(mfrow = c(1, 1))
plot(prune.house.c)
text(prune.house.c, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
house.c.test
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
prediction2 = exp(yhat[435:length(house.c.test)])
submission2 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission2,"submit2.csv",row.names = FALSE)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
prediction3 = exp(yhat[435:length(house.c.test)])
submission3 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission3,"submit3.csv",row.names = FALSE)
#Pruning the tree to have 4 terminal nodes.
prune.house.c = prune.tree(tree.house.c, best = 5)
par(mfrow = c(1, 1))
plot(prune.house.c)
text(prune.house.c, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
house.c.test
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
prediction2 = exp(yhat[435:length(house.c.test)])
submission2 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission2,"submit2.csv",row.names = FALSE)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
prediction3 = exp(yhat[435:length(house.c.test)])
submission3 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission3,"submit3.csv",row.names = FALSE)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
mean((yhat[1:434] - house.c.test[1:434])^2, na.rm = TRUE)
mean((yhat[1:434] - log(stepmodel.fullhouse.m.or$SalePrice)[1:434])^2, na.rm = TRUE)
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
house.c.test
plot(yhat[1:434], house.c.test[1:434])
abline(0, 1)
mean((yhat[1:434] - log(stepmodel.fullhouse.m.or$SalePrice)[1:434])^2, na.rm = TRUE)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[train, ])
yhat
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
#Calculating and assessing the MSE of the test data on the overall tree.
y = log(stepmodel.fullhouse.m.or$SalePrice)[-train]
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
house.c.test = stepmodel.fullhouse.m.or[-train, "SalePrice"]
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat[1:434] - y[1:434])^2, na.rm = TRUE)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[-train, ])
yhat
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat[1:434] - y[1:434])^2, na.rm = TRUE)
prediction3 = exp(yhat[435:length(house.c.test)])
#Calculating and assessing the MSE of the test data on the overall tree.
y = log(stepmodel.fullhouse.m.or$SalePrice)[train]
yhat= predict(tree.house.c, newdata = stepmodel.fullhouse.m.or[train, ])
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = stepmodel.fullhouse.m.or[train, ])
yhat
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat - y)^2, na.rm = TRUE)
tree.house.c = tree(log(SalePrice) ~ ., cormodel.fullhouse.m.or, subset = train)
summary(tree.house.c)
#Visually inspecting the regression tree.
plot(tree.house.c)
text(tree.house.c, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.house.c = cv.tree(tree.house.c)
par(mfrow = c(1, 2))
plot(cv.house.c$size, cv.house.c$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.house.c$k, cv.house.c$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.house.c = prune.tree(tree.house.c, best = 6)
par(mfrow = c(1, 1))
plot(prune.house.c)
text(prune.house.c, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
y = log(cormodel.fullhouse.m.or$SalePrice)[-train]
yhat= predict(tree.house.c, newdata = cormodel.fullhouse.m.or[-train, ])
yhat
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat[1:434] - y[1:434])^2, na.rm = TRUE)
prediction2 = exp(yhat[435:length(house.c.test)])
submission2 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission2,"submit2.csv",row.names = FALSE)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = cormodel.fullhouse.m.or[-train, ])
yhat
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat[1:434] - y[1:434])^2, na.rm = TRUE)
prediction3 = exp(yhat[435:length(house.c.test)])
submission3 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission3,"submit3.csv",row.names = FALSE)
#Calculating and assessing the MSE of the test data on the overall tree.
y = log(cormodel.fullhouse.m.or$SalePrice)[-train]
yhat= predict(tree.house.c, newdata = cormodel.fullhouse.m.or[-train, ])
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat[1:434] - y[1:434])^2, na.rm = TRUE)
prediction2 = exp(yhat[435:length(house.c.test)])
submission2 = data.frame(Id=house.test$Id, SalePrice=prediction2)
write.csv(submission2,"submit2.csv",row.names = FALSE)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = cormodel.fullhouse.m.or[-train, ])
plot(yhat[1:434], y[1:434])
abline(0, 1)
mean((yhat[1:434] - y[1:434])^2, na.rm = TRUE)
prediction3 = exp(yhat[435:length(house.c.test)])
#Calculating and assessing the MSE of the test data on the overall tree.
y = log(cormodel.fullhouse.m.or$SalePrice)[train]
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.house.c, newdata = cormodel.fullhouse.m.or[train, ])
mean((yhat - y)^2, na.rm = TRUE)
rf.house.c = randomForest(log(SalePrice) ~ ., stepmodel.fullhouse.m.or, subset = train, importance = TRUE)
boost.house.m = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 10)
#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.house.m)
#Letâ€™s make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.house.m, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(house.c.modeldata[1:439, ], apply((predmat[1:439,] - SalePrice)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best OOB error from the random forest.
abline(h = min(oob.err), col = "red")
prediction4 = exp(predmat[440:1898,which(berr==min(berr))])
submission4 = data.frame(Id=house.test$Id, SalePrice=prediction4)
write.csv(submission4,"submit4.csv", row.names = FALSE)
#Increasing the shrinkage parameter; a higher proportion of the errors are
#carried over.
set.seed(0)
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.0007) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(stepmodel.fullhouse.m.or[1:439,], apply((predmat2[1:439,] - SalePrice)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
min(berr)
min(berr2)
prediction5 = exp(predmat[440:1898,which(berr2==min(berr2))])
submission5 = data.frame(Id=house.test$Id, SalePrice=prediction5)
write.csv(submission5,"submit5.csv", row.names = FALSE)
dim(house.test)
1459-1893
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(house.c.modeldata[1:434, ], apply((predmat[1:434,] - SalePrice)^2, 2, mean))
#Visualizing the OOB error.
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
boost.house.m = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 10)
boost.house.m = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 8)
#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.house.m)
#Letâ€™s make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.house.m, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(house.c.modeldata[1:434, ], apply((predmat[1:434,] - SalePrice)^2, 2, mean))
y = stepmodel.fullhouse.m.or[-train, ]
berr = with(y[1:434,], apply((predmat[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best OOB error from the random forest.
abline(h = min(oob.err), col = "red")
prediction4 = exp(predmat[435:1893,which(berr==min(berr))])
submission4 = data.frame(Id=house.test$Id, SalePrice=prediction4)
write.csv(submission4,"submit4.csv", row.names = FALSE)
#Increasing the shrinkage parameter; a higher proportion of the errors are
#carried over.
set.seed(0)
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - SalePrice)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
min(berr)
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.007) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - SalePrice)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.01) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
min(berr)
min(berr2)
prediction5 = exp(predmat[435:1893,which(berr2==min(berr2))])
submission5 = data.frame(Id=house.test$Id, SalePrice=prediction5)
write.csv(submission5,"submit5.csv", row.names = FALSE)
berr = with(y[1:434,], apply((predmat[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.008) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
min(berr)
min(berr2)
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
boost.house.m2 = gbm(log(SalePrice) ~ ., data = stepmodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.01) #0.001
predmat2 = predict(boost.house.m2, newdata = stepmodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
min(berr)
min(berr2)
y = stepmodel.fullhouse.m.or[train, ]
boost.house.m2$train.error
mean(boost.house.m2$train.error)
library(randomForest)
#Fitting an initial random forest to the training subset.
set.seed(0)
rf.house.c = randomForest(log(SalePrice) ~ ., cormodel.fullhouse.m.or, subset = train, importance = TRUE)
rf.house.c
#The MSE and percent variance explained are based on out-of-bag estimates,
#yielding unbiased error estimates. The model reports that mtry = 4, which is
#the number of variables randomly chosen at each split. Since we have 13 overall
#variables, we could try all 13 possible values of mtry. We will do so, record
#the results, and make a plot.
#Varying the number of variables used at each step of the random forest procedure.
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(log(SalePrice) ~ ., data = cormodel.fullhouse.m.or[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
#Visualizing the OOB error.
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
#Can visualize a variable importance plot.
importance(rf.house.c)
varImpPlot(rf.house.c)
##################
#####Boosting#####
##################
library(gbm)
#Fitting 10,000 trees with a depth of 4.
set.seed(0)
boost.house.m = gbm(log(SalePrice) ~ ., data = cormodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 8)
#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.house.m)
=
#Letâ€™s make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.house.m, newdata = cormodel.fullhouse.m.or[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
y = cormodel.fullhouse.m.or[-train, ]
berr = with(y[1:434,], apply((predmat[1:434,] - log(SalePrice))^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best OOB error from the random forest.
abline(h = min(oob.err), col = "red")
prediction4 = exp(predmat[435:1893,which(berr==min(berr))])
boost.house.m2 = gbm(log(SalePrice) ~ ., data = cormodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.01) #0.001
predmat2 = predict(boost.house.m2, newdata = cormodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
mean(boost.house.m2$train.error)
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
min(berr)
min(berr2)
boost.house.m2 = gbm(log(SalePrice) ~ ., data = cormodel.fullhouse.m.or[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001) #0.001
predmat2 = predict(boost.house.m2, newdata = cormodel.fullhouse.m.or[-train, ], n.trees = n.trees)
berr2 = with(y[1:434,], apply((predmat2[1:434,] - log(SalePrice))^2, 2, mean))
mean(boost.house.m2$train.error)
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
colnames(stepmodel.fullhouse.m.or)
colnames(cormodel.fullhouse.m.or)
colnames(npmodel.fullhouse.m.or)
plot(ridge.bestlambdatrain,y.test,xlab="Prediction",ylab="Log Sale Price", main="Ridge Regression Fit")
abline(ridge.models.train)
summary(ridge.models.train)
abline(0,1,col=2,lty=2)
plot(ridge.bestlambdatrain,y.test,xlab="Prediction",ylab="Log Sale Price", main="Ridge Regression Fit")
abline(0,1,col=2,lty=2)
